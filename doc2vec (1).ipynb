{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#prepare"
      ],
      "metadata": {
        "id": "rodYWNJ1JYyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install gensim\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import cosine"
      ],
      "metadata": {
        "id": "xFThZLtHJYfH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data"
      ],
      "metadata": {
        "id": "NrGzlW0RI9o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('./Books.csv', encoding = 'utf-8')\n",
        "data.drop_duplicates(['title'], inplace = True)\n",
        "training_data = [TaggedDocument(words=data['overview'][i].split(), tags=[data['title'][i]]) for i in range(len(data))]"
      ],
      "metadata": {
        "id": "MVo9vgXaI9La"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train"
      ],
      "metadata": {
        "id": "p_8ut5abI7eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a doc2vec model\n",
        "model = Doc2Vec(vector_size=100,\n",
        "                epochs=40,\n",
        "                dm=0,            # PV-DBOW / default 1\n",
        "                dbow_words=1,    # w2v simultaneous with DBOW d2v / default 0\n",
        "                window=10,        # distance between the predicted word and context words\n",
        "                size=100,        # vector size\n",
        "                alpha=0.025,     # learning-rate\n",
        "                min_count=5,    # ignore with freq lower\n",
        "                min_alpha=0.025, # min learning-rate\n",
        "                workers=8,   # multi cpu\n",
        "                hs = 1,          # hierar chical softmax / default 0\n",
        "                negative = 10)\n",
        "\n",
        "model.build_vocab(training_data)\n",
        "model.train(training_data, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "metadata": {
        "id": "u5oiMqWkHltB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#inference"
      ],
      "metadata": {
        "id": "jX11nmcmJrDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Infer a vector for new data\n",
        "new_data = data['overview'][0]\n",
        "new_vector = model.infer_vector(new_data.split())\n",
        "\n",
        "# Calculate similarity scores for all pre-existing data\n",
        "similarity_scores = [(index, 1 - cosine(new_vector, model.docvecs[index].reshape(1, -1))) for index in range(len(model.docvecs))]\n",
        "\n",
        "k = 10\n",
        "top_k = [(index,score) for index, score in sorted(similarity_scores, key=lambda x: x[1], reverse=True)[:k]]\n",
        "\n",
        "recommended_docs = [(data['title'][index],round(score,3))  for index, score in top_k]\n",
        "recommended_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-zztj4wJnjP",
        "outputId": "bd970a97-3de4-4b89-e1ca-d04124bd6d44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('유도라 웰티의 소설작법', 0.461),\n",
              " ('번역의 일 - 번역이 없으면 세계도 없다', 0.452),\n",
              " ('우리말 맞춤법 띄어쓰기 - 모든 글쓰기의 시작과 완성', 0.44),\n",
              " ('사자소학 따라쓰기 - 손으로 쓰면서 마음에 새기는 인생 교과서', 0.399),\n",
              " ('마흔의 문장들', 0.395),\n",
              " ('자각의 연금술', 0.394),\n",
              " ('손잡지 않고 살아남은 생명은 없다 - 더불어 살아가기 위한 생명 이야기', 0.382),\n",
              " ('호모 데우스 - 미래의 역사', 0.382),\n",
              " ('사진으로 보는 우리 문화유산 ', 0.377),\n",
              " ('Magazine Makers - 독립 매거진을 창간해서 발행하는 5팀의 대화집', 0.377)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}