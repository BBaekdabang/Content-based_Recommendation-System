{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install gensim"
      ],
      "metadata": {
        "id": "3UKqbH2OPT0g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oNC4hY8iPO2D"
      },
      "outputs": [],
      "source": [
        "from gensim.models import doc2vec\n",
        "import pandas as pd\n",
        "from collections import namedtuple\n",
        "from tqdm.notebook import tqdm\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from time import time\n",
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('./Books.csv', encoding = 'utf-8')"
      ],
      "metadata": {
        "id": "v1VRfaQqPV2x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vectorizer = doc2vec.Doc2Vec(\n",
        "    dm=0,            # PV-DBOW / default 1\n",
        "    dbow_words=1,    # w2v simultaneous with DBOW d2v / default 0\n",
        "    window=10,        # distance between the predicted word and context words\n",
        "    size=100,        # vector size\n",
        "    alpha=0.025,     # learning-rate\n",
        "    seed=1234,\n",
        "    min_count=5,    # ignore with freq lower\n",
        "    min_alpha=0.025, # min learning-rate\n",
        "    workers=8,   # multi cpu\n",
        "    hs = 1,          # hierar chical softmax / default 0\n",
        "    negative = 10   # negative sampling / default 5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU-tRzFcQhnI",
        "outputId": "8912f15d-3c10-4017-f119-1afa66a1bd9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
            "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agg = data[['id', 'title', 'overview']]\n",
        "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
        "tagged_train_docs = [TaggedDocument((c), [d]) for d, c in agg[['title', 'overview']].values]"
      ],
      "metadata": {
        "id": "HHQlg-ddQoSV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vectorizer.build_vocab(tagged_train_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHfs6RFOQtua",
        "outputId": "4e2fd8db-fdbd-4560-8d16-00364a834960"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.doc2vec:Each 'words' should be a list of words (usually unicode strings). First 'words' here is instead plain <class 'str'>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time()\n",
        "\n",
        "for epoch in range(5):\n",
        "    doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n",
        "    doc_vectorizer.alpha -= 0.002 # decrease the learning rate\n",
        "    doc_vectorizer.min_alpha = doc_vectorizer.alpha # fix the learning rate, no decay\n",
        "\n",
        "#doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n",
        "end = time()\n",
        "print(\"During Time: {}\".format(end-start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnb6gyWFQwUg",
        "outputId": "0620021c-93b1-4785-98bc-3f9eadbfe9a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-dd0958075b5c>:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "During Time: 496.4702091217041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vectorizer.docvecs.most_similar('내가 틀릴 수도 있습니다 - 숲속의 현자가 전하는 마지막 인생 수업', topn = 10 )"
      ],
      "metadata": {
        "id": "LL7P5E3d7Szs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64b03d7-8845-4236-854d-e0fea5fef613"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('나는 나무처럼 살고 싶다 - 30년간 아픈 나무들을 돌봐 온 나무 의사 우종영이 나무에게 배운 단단한 삶의 지혜 35',\n",
              "  0.4319668114185333),\n",
              " ('몰입의 즐거움 - [ 양장 ]', 0.4260326623916626),\n",
              " ('어른 그림책 여행 - 내 마음을 둘러보고 싶을 때', 0.4240935444831848),\n",
              " ('삶을 여행하는 초심자를 위한 - 죽음 가이드북', 0.4179852604866028),\n",
              " ('마음의 요가 - 인도 최고의 지성과 영성, 비베카난다의 말', 0.3797704875469208),\n",
              " ('책의 맛 - 로제 그르니에가 펼쳐 보이는 문학의 세계', 0.37742409110069275),\n",
              " ('아비투스 - 인간의 품격을 결정하는 7가지 자본', 0.37696927785873413),\n",
              " ('길 위의 철학자 - 떠돌이 철학자의 삶에 관한 에피소드', 0.3743991255760193),\n",
              " ('오후의 글쓰기 - 자발적 글쓰기를 시작하는 어른을 위한 따뜻한 문장들', 0.36943519115448),\n",
              " ('모든 맛에는 이유가 있다 - 인문학에 과학으로 감칠맛을 더한 가장 지적인 파인다이닝', 0.3643539845943451)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}